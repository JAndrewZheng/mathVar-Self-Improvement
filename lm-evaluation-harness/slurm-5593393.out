2025-10-22:12:39:16 INFO     [__main__:450] Selected Tasks: ['aime']
2025-10-22:12:39:16 INFO     [evaluator:202] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
2025-10-22:12:39:16 WARNING  [evaluator:214] generation_kwargs: {'max_gen_toks': 32767} specified through cli, these settings will update set parameters in yaml tasks. Ensure 'do_sample=True' for non-greedy decoding!
2025-10-22:12:39:16 INFO     [evaluator:240] Initializing hf model, with arguments: {'pretrained': 'Qwen/Qwen2.5-Math-7B'}
2025-10-22:12:39:16 WARNING  [accelerate.utils.other:513] Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
2025-10-22:12:39:16 INFO     [models.huggingface:156] Using device 'cuda:0'
2025-10-22:12:39:17 INFO     [models.huggingface:423] Model parallel was set to False, max memory was not set, and device map was set to {'': 'cuda:0'}
`torch_dtype` is deprecated! Use `dtype` instead!
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:01,  1.56it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.56it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:01<00:00,  1.52it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.59it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.57it/s]
2025-10-22:12:39:20 INFO     [evaluator:305] aime: Using gen_kwargs: {'until': ['Question:', '</s>', '<|im_end|>', '<|eot_id|>'], 'do_sample': False, 'temperature': 0.0, 'max_gen_toks': 32767}
2025-10-22:12:39:20 INFO     [evaluator:320] num_fewshot has been set to 0 for aime in its config. Manual configuration will be ignored.
2025-10-22:12:39:20 WARNING  [evaluator:480] Chat template formatting change affects loglikelihood and multiple-choice tasks. See docs/chat-template-readme.md for details.
2025-10-22:12:39:20 INFO     [api.task:434] Building contexts for aime on rank 0...
  0%|          | 0/933 [00:00<?, ?it/s] 25%|██▌       | 236/933 [00:00<00:00, 2359.81it/s] 56%|█████▌    | 521/933 [00:00<00:00, 2646.53it/s] 86%|████████▋ | 807/933 [00:00<00:00, 2741.09it/s]100%|██████████| 933/933 [00:00<00:00, 2707.25it/s]
2025-10-22:12:39:21 INFO     [evaluator:574] Running generate_until requests
Running generate_until requests:   0%|          | 0/933 [00:00<?, ?it/s]Traceback (most recent call last):
  File "/fs/nexus-scratch/azheng15/miniconda3/envs/mathVarRL/bin/lm_eval", line 7, in <module>
    sys.exit(cli_evaluate())
             ~~~~~~~~~~~~^^
  File "/fs/nexus-scratch/azheng15/research/self-improvement/Var-self-improvement/lm-evaluation-harness/lm_eval/__main__.py", line 459, in cli_evaluate
    results = evaluator.simple_evaluate(
        model=args.model,
    ...<25 lines>...
        **request_caching_args,
    )
  File "/fs/nexus-scratch/azheng15/research/self-improvement/Var-self-improvement/lm-evaluation-harness/lm_eval/utils.py", line 458, in _wrapper
    return fn(*args, **kwargs)
  File "/fs/nexus-scratch/azheng15/research/self-improvement/Var-self-improvement/lm-evaluation-harness/lm_eval/evaluator.py", line 357, in simple_evaluate
    results = evaluate(
        lm=lm,
    ...<12 lines>...
        confirm_run_unsafe_code=confirm_run_unsafe_code,
    )
  File "/fs/nexus-scratch/azheng15/research/self-improvement/Var-self-improvement/lm-evaluation-harness/lm_eval/utils.py", line 458, in _wrapper
    return fn(*args, **kwargs)
  File "/fs/nexus-scratch/azheng15/research/self-improvement/Var-self-improvement/lm-evaluation-harness/lm_eval/evaluator.py", line 585, in evaluate
    resps = getattr(lm, reqtype)(cloned_reqs)
  File "/fs/nexus-scratch/azheng15/research/self-improvement/Var-self-improvement/lm-evaluation-harness/lm_eval/models/huggingface.py", line 1445, in generate_until
    assert max_ctx_len > 0, (
           ^^^^^^^^^^^^^^^
AssertionError: Invalid configuration: requested max tokens to generate (32767) must be less than model's maximum sequence length (4096).
Running generate_until requests:   0%|          | 0/933 [00:00<?, ?it/s]
